# Summary of papers in Adversarial Settings

---
## Background of Deep Learning Security and applications of adversarial attack
:white_check_mark: 000.Machine Learning in Adversarial Settings-PPT

:white_check_mark: 0603.Can Machine Learning Be Secure

:white_check_mark: 1100.Adversarial Machine Learning

:white_check_mark: 1610.Accessorize to a Crime : Real and Stealthy Attacks on State-of-the-Art Face Recognition

:white_check_mark: 1707.NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles

:white_check_mark: 1710.Standard detectors aren't (currently) fooled by physical adversarial stop signs

---
## Existence of adversarial examples
:white_check_mark: 1500.Fundamental limits on adversarial robustness

:white_check_mark: 1503.Explaining and Harnessing Adversarial Examples

:white_check_mark: 1608.A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples

:white_check_mark: 1608.Robustness of classifiers:from adversarial to random noise

:white_check_mark: 1705.Analysis of universal adversarial perturbations

:white_check_mark: 1801.High Dimensional Spaces, Deep Learning and Adversarial Examples

---
## Attack Algorithms
:white_check_mark: 1400.Evasion attacks against machine learning at test time

:white_check_mark: 1402.Intriguing properties of neural networks

:white_check_mark: 1503.Explaining and Harnessing Adversarial Examples

:white_check_mark: 1504.Deep neural networks are easily fooled High confidence predictions for unrecognizable images

:white_check_mark: 1507.Distributional Smoothing with Virtual Adversarial Training

:white_check_mark: 1507.Manitest-Are classifiers really invariant

:white_check_mark: 1510.Exploring the space of adversarial images

:white_check_mark: 1510.The Limitations of Deep Learning in Adversarial Settings

:white_check_mark: 1601.Adversarial Perturbations Against Deep Neural Networks for Malware Classification

:white_check_mark: 1602.Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples

:white_check_mark: 1605.Transferability in Machine Learning from Phenomena to Black-Box Attacks using Adversarial Samples

:white_check_mark: 1607.DeepFool_ A Simple and Accurate Method to Fool Deep Neural Networks

:white_check_mark: 1608.Stealing Machine Learning Models via Prediction APIs

:white_check_mark: 1610.DeepDGA-Adversarially Tuned Domain Generation and Detection

:white_check_mark: 1611.Delving into Transferable Adversarial Examples and Black-box Attacks

:white_check_mark: 1612.Simple Black-Box Adversarial Perturbations for Deep Networks

:white_check_mark: 1700.Concrete Problems for Autonomous Vehicle Safety_ Advantages of Bayesian Deep Learning

:white_check_mark: 1701.Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks

:white_check_mark: 1702.Adversarial Attacks on Neural Network Policies

:white_check_mark: 1702.Adversarial examples for generative models

:white_check_mark: 1702.Adversarial examples in the physical world

:white_check_mark: 1702.Adversarial machine learning at scale

:white_check_mark: 1702.Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN

:white_check_mark: 1702.Towards Deep Learning Models Resistant to Adversarial Attacks

:white_check_mark: 1703.Adversarial Transformation Networks: Learning to Generate Adversarial Examples

:white_check_mark: 1703.Generative Poisoning Attack Method Against Neural Networks

:white_check_mark: 1703.Notes on Adversarial Examples

:white_check_mark: 1703.Tactics of Adversarial Attack on Deep Reinforcement Learning Agents

:white_check_mark: 1703.Towards Evaluating the Robustness of Neural Networks

:white_check_mark: 1703.Universal adversarial perturbations

:white_check_mark: 1705.Analysis of universal adversarial perturbations

:white_check_mark: 1705.Ensemble Adversarial Training Attacks and Defenses

:white_check_mark: 1705.Generative Adversarial Trainer Defense to Adversarial Perturbations with GAN

:white_check_mark: 1705.Stabilizing Adversarial Nets With Prediction Methods

:white_check_mark: 1707.APE-GAN-- Adversarial Perturbation Elimination with GAN

:white_check_mark: 1707.Evading Machine Learning Malware Detection

:white_check_mark: 1707.Fast Feature Fool-A data independent approach to universal adversarial perturbations

:white_check_mark: 1707.Robust Physical-World Attacks on Machine Learning Models

:white_check_mark: 1707.Synthesizing Robust Adversarial Examples

:white_check_mark: 1708.Machine Learning as an Adversarial Service Learning Black-Box Adversarial Examples

:white_check_mark: 1708.Proof of Work Without All the Work

:white_check_mark: 1709.Can you fool AI with adversarial examples on a visual Turing test

:white_check_mark: 1709.EAD Elastic Net Attacks to Deep Neural Networks via Adversarial Examples

:white_check_mark: 1709.Ground-Truth Adversarial Examples

:white_check_mark: 1711.Security Risks in Deep Learning Implementations

:white_check_mark: 1712.Adversarial Patch

:white_check_mark: 1712.Robust Deep Reinforcement Learning with Adversarial Attacks

:white_check_mark: 1712.Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning

:white_check_mark: 1712.Where Classification Fails, Interpretation Rises

:white_check_mark: 1801.Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning

:white_check_mark: 1801.LaVAN-Localized and Visible Adversarial Noise

---
## Defence Strategies
:white_check_mark: 1603.Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks

:white_check_mark: 1604.Improving the robustness of deep neural networks via stability training

:white_check_mark: 1605.Adversarial Training Methods for Semi-Supervised Text Classification

:white_check_mark: 1607.Defensive Distillation is Not Robust to Adversarial Examples

:white_check_mark: 1608.A study of the effect of JPG compression on adversarial images

:white_check_mark: 1610.Adversary Resistant Deep Neural Networks with an Application to Malware Detection

:white_check_mark: 1703.Biologically inspired protection of deep networks from adversarial attacks

:white_check_mark: 1704.Enhancing Robustness of Machine Learning Systems via Data Transformations

:white_check_mark: 1704.Feature Squeezing Detecting Adversarial Examples in Deep Neural Networks

:white_check_mark: 1705.Detecting Adversarial Examples in Deep Neural Networks

:white_check_mark: 1705.Extending Defensive Distillation

:white_check_mark: 1705.Feature Squeezing Mitigates and Detects Carlini Wagner Adversarial Examples

:white_check_mark: 1705.Generative Adversarial Trainer Defense to Adversarial Perturbations with GAN

:white_check_mark: 1705.MagNet-a Two-Pronged Defense against Adversarial Examples

:white_check_mark: 1706.Adversarial Example Defenses Ensembles of Weak Defenses are not Strong

:white_check_mark: 1707.AE-GAN adversarial eliminating with GAN

:white_check_mark: 1707.APE-GAN-- Adversarial Perturbation Elimination with GAN


---
## Related survey researches
:white_check_mark: 1606.Concrete Problems in AI Safety

:white_check_mark: 1610.SoK Towards the Science of Security and Privacy in Machine Learning

:white_check_mark: 1611.Towards the Science of Security and Privacy in Machine Learning

:white_check_mark: 1707.A Survey on Resilient Machine Learning

:white_check_mark: 1712.Adversarial Examples-Attacks and Defenses for Deep Learning

:white_check_mark: 1801.Threat of Adversarial Attacks on Deep Learning in Computer Vision-A Survey

:white_check_mark: 1802.Adversarial Risk and the Dangers of Evaluating Against Weak Attacks

---
## Detection papers
:white_check_mark: 1612.Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics

:white_check_mark: 1702.On Detecting Adversarial Perturbations

:white_check_mark: 1702.On the (Statistical) Detection of Adversarial Examples

:white_check_mark: 1703.Blocking Transferability of Adversarial Examples in Black-Box Learning Systems

:white_check_mark: 1703.Detecting Adversarial Samples from Artifacts

:white_check_mark: 1704.SafetyNet-Detecting and Rejecting Adversarial Examples Robustly

:white_check_mark: 1705.Adversarial Examples Are Not Easily Detected-Bypassing Ten Detection Methods

:white_check_mark: 1712.Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser

:white_check_mark: 1803.Detecting Adversarial Examples via Neural Fingerprinting
